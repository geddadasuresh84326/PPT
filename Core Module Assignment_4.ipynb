{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86a99ad",
   "metadata": {},
   "source": [
    "## General Linear Model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc9509",
   "metadata": {},
   "source": [
    "### 1. What is the purpose of the General Linear Model (GLM)?\n",
    "- GLM models allow us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear. This is made possible by using a link function, which links the response variable to a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054c96b",
   "metadata": {},
   "source": [
    "### 2. What are the key assumptions of the General Linear Model?\n",
    "1. linearity\n",
    "2. homoskedasticity (constant variance)\n",
    "3. normality\n",
    "4. independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa450e",
   "metadata": {},
   "source": [
    "### 3. How do you interpret the coefficients in a GLM?\n",
    "- The GLM coefficients only show the multiplicative change in odds ratio. so if p1 is the risk of getting a high score for black defendants and p0 is the risk of getting a high score for white defendants, then exp(0.47721) shows (p1/(1-p1))/(p0/(1-p0))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afdb2d",
   "metadata": {},
   "source": [
    "### 4. What is the difference between a univariate and multivariate GLM?\n",
    "####  Univariate :- \n",
    "- Analysis on one variable\n",
    "#### multivariate :- \n",
    "- Analysis on more that two variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bdb2d",
   "metadata": {},
   "source": [
    "### . Explain the concept of interaction effects in a GLM.\n",
    "- the existence of an interaction means that the effect of one variable depends on the value of the other variable with which it interacts. If there isn't an interaction, then the value of the other variable doesn't matter. This is easiest to understand in the case of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c3eee",
   "metadata": {},
   "source": [
    "### 6. How do you handle categorical predictors in a GLM?\n",
    "- Label Encoding Label encoding assigns each unique value to a different integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0939acc",
   "metadata": {},
   "source": [
    "### 7. What is the purpose of the design matrix in a GLM?\n",
    "- The purpose of the design matrix is to allow models that further constrain parameter sets. These constraints provide additional flexibility in modeling and allows researchers to build models that cannot be derived using the simple PIMs in ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df36b9",
   "metadata": {},
   "source": [
    "### 8. How do you test the significance of predictors in a GLM?\n",
    "- T-tests are used in linear regression to determine if a particular variable is statistically significant in the model. A statistically significant variable is one that has a strong relationship with the dependent variable and contributes significantly to the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493adc8",
   "metadata": {},
   "source": [
    "### 9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "- Type I sum of squares are “sequential.” In essence the factors are tested in the order they are listed in the model. \n",
    "- Type II Sums of Squares are not sequential, so the order of specification does not matter.\n",
    "- Type III sum of squares are “partial.” In essence, every term in the model is tested in light of every other term in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644258f2",
   "metadata": {},
   "source": [
    "### 10. Explain the concept of deviance in a GLM.\n",
    "- The deviance is used to compare two models – in particular in the case of generalized linear models (GLM) where it has a similar role to residual sum of squares from ANOVA in linear models (RSS). Suppose in the framework of the GLM, we have two nested models, M1 and M2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f034ee5",
   "metadata": {},
   "source": [
    "## Regression:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb0787",
   "metadata": {},
   "source": [
    "### 11. What is regression analysis and what is its purpose?\n",
    "- We use regression analysis to predict continuous numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e14c3d",
   "metadata": {},
   "source": [
    "### 12. What is the difference between simple linear regression and multiple linear regression?\n",
    "- In simple linear regression we do predictions using one independent feature and one dependent feature\n",
    "- In multiple linear regression we do predictions using more than one independent feature and one dependent feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ec57d",
   "metadata": {},
   "source": [
    "### 13. How do you interpret the R-squared value in regression?\n",
    "- In linear regression models, r squared interpretation is a goodness-fit-measure. It takes into account the strength of the relationship between the model and the dependent variable. Its convenience is measured on a scale of 0 – 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2104b55",
   "metadata": {},
   "source": [
    "### 14. What is the difference between correlation and regression?\n",
    "\n",
    "- Correlation is a statistical measure that determines the association or co-relationship between two variables.\n",
    "- Regression describes how to numerically relate an independent variable to the dependent variable. To represent a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd31eb",
   "metadata": {},
   "source": [
    "### 15. What is the difference between the coefficients and the intercept in regression?\n",
    "- The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ee8f4",
   "metadata": {},
   "source": [
    "### 16. How do you handle outliers in regression analysis?\n",
    "- Dropping the outliers - it prevents skewing of the data.\n",
    "- Cap them - ie. define a max/min point and assign that value to the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ea235",
   "metadata": {},
   "source": [
    "### 17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "- Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a96f2",
   "metadata": {},
   "source": [
    "### 18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "- Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values. When running a regression analysis, heteroskedasticity results in an unequal scatter of the residuals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41cff4",
   "metadata": {},
   "source": [
    "### 19. How do you handle multicollinearity in regression analysis?\n",
    "\n",
    "1. Remove some of the highly correlated independent variables.\n",
    "2. Linearly combine the independent variables, such as adding them together.\n",
    "3. Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a2869",
   "metadata": {},
   "source": [
    "### 20. What is polynomial regression and when is it used?\n",
    "- A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f7000",
   "metadata": {},
   "source": [
    "## Loss function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc270a5",
   "metadata": {},
   "source": [
    "### 21. What is a loss function and what is its purpose in machine learning?\n",
    "- At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a2de8",
   "metadata": {},
   "source": [
    "### 22. What is the difference between a convex and non-convex loss function?\n",
    "\n",
    "- A convex function is having one global minima and no local minimas\n",
    "- A non-convex function may have any no.of local minimas and one global minima\n",
    "- A convex function is one in which a line drawn between any two points on the graph lies on the graph or above it. There is only one requirement. A non-convex function is one in which a line drawn between any two points on the graph may cross additional points. It was described as “wavy.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8371dcd",
   "metadata": {},
   "source": [
    "### 23. What is mean squared error (MSE) and how is it calculated?\n",
    "- Mean squared error (MSE) measures the amount of error in statistical models. It assesses the average squared difference between the observed and predicted values. When a model has no error, the MSE equals zero. As model error increases, its value increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9dd7f",
   "metadata": {},
   "source": [
    "### 24. What is mean absolute error (MAE) and how is it calculated?\n",
    "- Mean Absolute Error (MAE) is calculated by taking the summation of the absolute difference between the actual and calculated values of each observation over the entire array and then dividing the sum obtained by the number of observations in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef0bb5",
   "metadata": {},
   "source": [
    "### 25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "- Binary cross entropy, also referred to as logarithmic loss or log loss, is a metric used to evaluate models by measuring the extent of incorrect labeling of data classes. It penalizes the model for deviations in probability that result in misclassification of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a6926",
   "metadata": {},
   "source": [
    "### 26. How do you choose the appropriate loss function for a given problem?\n",
    "- It depends on whether we are solving regression problem or classification problem\n",
    "- Mean Squared error.\n",
    "- Mean Absolute Error.\n",
    "- Log-Likelihood Loss.\n",
    "- Hinge Loss.\n",
    "- Huber Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a240e49",
   "metadata": {},
   "source": [
    "### 27. Explain the concept of regularization in the context of loss functions.\n",
    "- During the L2 regularization the loss function of the neural network as extended by a so-called regularization term, which is called here Ω. The regularization term Ω is defined as the Euclidean Norm (or L2 norm) of the weight matrices, which is the sum over all squared weight values of a weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b741cf8",
   "metadata": {},
   "source": [
    "### 28. What is Huber loss and how does it handle outliers?\n",
    "\n",
    "- Compared with MSE, Huber Loss is less sensitive to outliers as if the loss is too much it changes quadratic equation to linear and hence is a combination of both MSE and MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba2f95",
   "metadata": {},
   "source": [
    "### 29. What is quantile loss and when is it used?\n",
    "\n",
    "- The most popular quantile is the median, or the 50th percentile, and in this case the quantile loss is simply the sum of absolute errors. Other quantiles could give endpoints of a prediction interval; for example a middle-80-percent range is defined by the 10th and 90th percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8faf6f",
   "metadata": {},
   "source": [
    "### 30. What is the difference between squared loss and absolute loss?\n",
    "- For square loss, you will choose the estimated mean of y0, as the true mean minimizes square loss on average (where the average is taken across random samples of y0 subject to x=x0).\n",
    "- For absolute loss, you will choose the estimated median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d34e23",
   "metadata": {},
   "source": [
    "## Optimizer (GD):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2476759",
   "metadata": {},
   "source": [
    "### 31. What is an optimizer and what is its purpose in machine learning?\n",
    "- An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd88f6",
   "metadata": {},
   "source": [
    "### 32. What is Gradient Descent (GD) and how does it work?\n",
    "- Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadda05",
   "metadata": {},
   "source": [
    "### 33. What are the different variations of Gradient Descent?\n",
    "1. batch gradient descent\n",
    "2. stochastic gradient descent \n",
    "3. mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c70f",
   "metadata": {},
   "source": [
    "### 34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "\n",
    "- The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215f7e7",
   "metadata": {},
   "source": [
    "### 35. How does GD handle local optima in optimization problems?\n",
    "- Local optimization involves finding the optimal solution for a specific region of the search space, or the global optima for problems with no local optima. Global optimization involves finding the optimal solution on problems that contain local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6646864",
   "metadata": {},
   "source": [
    "### 36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "- Both algorithms are quite similar. The only difference comes while iterating. In Gradient Descent, we consider all the points in calculating loss and derivative, while in Stochastic gradient descent, we use single point in loss function and its derivative randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b391d",
   "metadata": {},
   "source": [
    "### 37. Explain the concept of batch size in GD and its impact on training.\n",
    "- The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66b2a3",
   "metadata": {},
   "source": [
    "### 38. What is the role of momentum in optimization algorithms?\n",
    "- Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee2635",
   "metadata": {},
   "source": [
    "### 39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "- Mini Batch Gradient Descent and Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75826cc",
   "metadata": {},
   "source": [
    "### 40. How does the learning rate affect the convergence of GD?\n",
    "- If the learning rate is very large we will skip the optimal solution. If it is too small we will need too many iterations to converge to the best values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff93300",
   "metadata": {},
   "source": [
    "## Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67b1ec",
   "metadata": {},
   "source": [
    "### 41. What is regularization and why is it used in machine learning?\n",
    "- Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b03044",
   "metadata": {},
   "source": [
    "### 42. What is the difference between L1 and L2 regularization?\n",
    "\n",
    "- L1 regularization penalizes the sum of absolute values of the weights\n",
    "- L2 regularization penalizes the sum of squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8e25a",
   "metadata": {},
   "source": [
    "### 43. Explain the concept of ridge regression and its role in regularization.\n",
    "\n",
    "- Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb38d46",
   "metadata": {},
   "source": [
    "### 44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "- The elastic net is a linear regression regularization technique that combines both the L1 (Lasso) and L2 (Ridge) regularization penalties. It is particularly useful when dealing with datasets that have high collinearity or when there are more predictors than observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4100354",
   "metadata": {},
   "source": [
    "### 45. How does regularization help prevent overfitting in machine learning models?\n",
    "\n",
    "- Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920ef60",
   "metadata": {},
   "source": [
    "### 46. What is early stopping and how does it relate to regularization?\n",
    "- In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81f9b5",
   "metadata": {},
   "source": [
    "### 47. Explain the concept of dropout regularization in neural networks.\n",
    "- Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ffc7c",
   "metadata": {},
   "source": [
    "### 48. How do you choose the regularization parameter in a model?\n",
    "- on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb5d15",
   "metadata": {},
   "source": [
    "### 49. What is the difference between feature selection and regularization?\n",
    "\n",
    "- Feature selection, also known as feature subset selection, variable selection, or attribute selection.This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference.\n",
    "- Regularization, where we are constraining the solution space while doing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd319bce",
   "metadata": {},
   "source": [
    "### 50. What is the trade-off between bias and variance in regularized models?\n",
    "- Bias Variance Tradeoff\n",
    "- If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31dc7f",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a988e",
   "metadata": {},
   "source": [
    "### 51. What is Support Vector Machines (SVM) and how does it work?\n",
    "- SVM is a supervised learning model with related learning algorithms that model data structure used for regression analysis. However, SVM may be applied not only to classification tasks, but also to the tasks of regression, which can model relationships between predictor variables and output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a159097",
   "metadata": {},
   "source": [
    "### 52. How does the kernel trick work in SVM?\n",
    "- Kernel trick allows the inner product of mapping function instead of the data points. The trick is to identify the kernel functions which can be represented in place of the inner product of mapping functions. Kernel functions allow easy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012d780",
   "metadata": {},
   "source": [
    "### 53. What are support vectors in SVM and why are they important?\n",
    "- Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eccb2",
   "metadata": {},
   "source": [
    "### 54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "- it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd19db",
   "metadata": {},
   "source": [
    "### 55. How do you handle unbalanced datasets in SVM?\n",
    "- Perhaps the simplest and most common extension to SVM for imbalanced classification is to weight the C value in proportion to the importance of each class. To accommodate these factors in SVMs an instance-level weighted modification was proposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34de62d",
   "metadata": {},
   "source": [
    "### 56. What is the difference between linear SVM and non-linear SVM?\n",
    "- Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data.\n",
    "- Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9da1c1",
   "metadata": {},
   "source": [
    "### 57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "- C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44fc98",
   "metadata": {},
   "source": [
    "### 58. Explain the concept of slack variables in SVM.\n",
    "- Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41975c3",
   "metadata": {},
   "source": [
    "### 59. What is the difference between hard margin and soft margin in SVM?\n",
    "- Hard margin SVM does not allow any misclassification to happen. In case our data is non-separable/ nonlinear then the Hard margin SVM will not return any hyperplane as it will not be able to separate the data. Hence this is where Soft Margin SVM comes to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bae712",
   "metadata": {},
   "source": [
    "### 60. How do you interpret the coefficients in an SVM model?\n",
    "-  The weights represent this hyperplane, by giving you the coordinates of a vector which is orthogonal to the hyperplane - these are the coefficients given by svm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477ecda",
   "metadata": {},
   "source": [
    "## Decision Trees:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c776d3f",
   "metadata": {},
   "source": [
    "### 61. What is a decision tree and how does it work?\n",
    "- A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7a46e",
   "metadata": {},
   "source": [
    "### 62. How do you make splits in a decision tree?\n",
    "- By calculating Gini and Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d211aaf",
   "metadata": {},
   "source": [
    "### 63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "- In the context of Decision Trees, entropy is a measure of disorder or impurity in a node. Thus, a node with more variable composition, such as 2Pass and 2 Fail would be considered to have higher Entropy than a node which has only pass or only fail. The maximum level of entropy or disorder is given by 1 and minimum entropy is given by a value 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293c964",
   "metadata": {},
   "source": [
    "### 64. Explain the concept of information gain in decision trees.\n",
    "\n",
    "- Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35406dad",
   "metadata": {},
   "source": [
    "### 65. How do you handle missing values in decision trees?\n",
    "- It will consider the missing values by taking the majority of the K nearest values. The random forest also is robust to categorical data with missing values. Many decision tree-based algorithms like XGBoost, Catboost support data with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4a456",
   "metadata": {},
   "source": [
    "### 66. What is pruning in decision trees and why is it important?\n",
    "\n",
    "- Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8219d15",
   "metadata": {},
   "source": [
    "### 67. What is the difference between a classification tree and a regression tree?\n",
    "- There are 2 types of Decision trees: Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c94b8",
   "metadata": {},
   "source": [
    "### 68. How do you interpret the decision boundaries in a decision tree?\n",
    "- Decision boundary of a decision tree is determined by overlapping orthogonal half-planes (representing the result of each subsequent decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e97846",
   "metadata": {},
   "source": [
    "### 69. What is the role of feature importance in decision trees?\n",
    "- Feature importance is a common way to make interpretable machine learning models and also explain existing models. That enables to see the big picture while taking decisions and avoid black box models. We've mentioned feature importance for linear regression and decision trees before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a05648",
   "metadata": {},
   "source": [
    "### 70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "- Using one decision tree is can be problematic and might not be stable enough; however, using multiple decision trees and combining their results will do great. Combining multiple classifiers in a prediction model is called ensembling. The simple rule of ensemble methods is to reduce the error by reducing the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada2d9c",
   "metadata": {},
   "source": [
    "## Ensemble Techniques:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1e6c3",
   "metadata": {},
   "source": [
    "### 71. What are ensemble techniques in machine learning?\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Stacking\n",
    "4. Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59329a4d",
   "metadata": {},
   "source": [
    "### 72. What is bagging and how is it used in ensemble learning?\n",
    "- Bagging also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c32b2",
   "metadata": {},
   "source": [
    "### 73. Explain the concept of bootstrapping in bagging.\n",
    "- Bootstrap means that instead of training on all the observations, each tree of RF is trained on a subset of the observations. The chosen subset is called the bag, and the remaining are called Out of Bag samples. Multiple trees are trained on different bags, and later the results from all the trees are aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cca8e",
   "metadata": {},
   "source": [
    "### 74. What is boosting and how does it work?\n",
    "- Boosting transforms weak decision trees (called weak learners) into strong learners. Each new tree is built considering the errors of previous trees. In both bagging and boosting, the algorithms use a group (ensemble) of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94b0fc",
   "metadata": {},
   "source": [
    "### 75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "- AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3662fdb",
   "metadata": {},
   "source": [
    "### 76. What is the purpose of random forests in ensemble learning?\n",
    "- Random Forest is a popular machine learning algorithm used for classification and regression tasks due to its high accuracy, robustness, feature importance, versatility, and scalability. Random Forest reduces overfitting by averaging multiple decision trees and is less sensitive to noise and outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0e283",
   "metadata": {},
   "source": [
    "### 77. How do random forests handle feature importance?\n",
    "- The final feature importance, at the Random Forest level, is it's average over all the trees. The sum of the feature's importance value on each trees is calculated and divided by the total number of trees: RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845702",
   "metadata": {},
   "source": [
    "### 78. What is stacking in ensemble learning and how does it work?\n",
    "- Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d99cbe",
   "metadata": {},
   "source": [
    "### 79. What are the advantages and disadvantages of ensemble techniques?\n",
    "#### Ensemble methods offer several advantages \n",
    "- improved accuracy \n",
    "- performance, especially for complex and noisy problems.\n",
    "#### Disadvantages of Ensemble learning\n",
    "- Ensembling is less interpretable, the output of the ensembled model is hard to predict and explain. ...\n",
    "- The art of ensembling is hard to learn and any wrong selection can lead to lower predictive accuracy than an individual model.\n",
    "- Ensembling is expensive in terms of both time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f091e",
   "metadata": {},
   "source": [
    "### 80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "#### The Algorithm\n",
    "- Step 1 : Find the KS of individual models. ...\n",
    "- Step 2: Index all the models for easy access. ...\n",
    "- Step 3: Choose the first two models as the initial selection and set a correlation limit. ...\n",
    "- Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
