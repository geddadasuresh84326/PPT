{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2f44f0",
   "metadata": {},
   "source": [
    "## Naive Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7f2fc",
   "metadata": {},
   "source": [
    "### 1. What is the Naive Approach in machine learning?\n",
    "- A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b2685",
   "metadata": {},
   "source": [
    "### 2. Explain the assumptions of feature independence in the Naive Approach.\n",
    "- The key difference is that naive bayes assumes that features are independent of each other and there is no correlation between features. However, this is not the case in real life. This naive assumption of features being uncorrelated is the reason why this algorithm is called “naive”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33d35d",
   "metadata": {},
   "source": [
    "### 3. How does the Naive Approach handle missing values in the data?\n",
    "- Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b98185",
   "metadata": {},
   "source": [
    "### 4. What are the advantages and disadvantages of the Naive Approach?\n",
    "- The advantage is that it is inexpensive to develop, store data, and operate.\n",
    "- The disadvantage is that it does not consider any possible causal relationships that underly the forecasted variable. This model adds the latest observed absolute period -to-period change to the most recent observed level of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a797a",
   "metadata": {},
   "source": [
    "### 5. Can the Naive Approach be used for regression problems? If yes, how?\n",
    "- standard naive Bayes applied to regression problems by discretizing the target value performs similarly badly. We then present empirical evidence that isolates naive Bayes' independence assumption as the culprit for its poor performance in the regression setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8f22",
   "metadata": {},
   "source": [
    "### 6. How do you handle categorical features in the Naive Approach?\n",
    "- Given the categorical features (not real-valued data) along with categorical class labels, Naive Bayes computes likelihood for each category from every feature with respect to each category of class labels. Thus, it will choose a specific category of a class label whose likelihood is maximum. That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31a5fb",
   "metadata": {},
   "source": [
    "### 7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
    "- Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9688b",
   "metadata": {},
   "source": [
    "### 8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
    "- The threshold governs the choice to turn a projected probability or scores into a class label. For normalized projected probabilities in the range of 0 to 1, the threshold is set to 0.5 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b246f1",
   "metadata": {},
   "source": [
    "### 9. Give an example scenario where the Naive Approach can be applied.\n",
    "- sentimental analysis\n",
    "- classifying new articles\n",
    "- spam filtration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3171b4",
   "metadata": {},
   "source": [
    "## KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a30e1c",
   "metadata": {},
   "source": [
    "### 10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
    "- The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7914e17",
   "metadata": {},
   "source": [
    "### 11. How does the KNN algorithm work?\n",
    "\n",
    "- First, the distance between the new point and each training point is calculated.\n",
    "- The closest k data points are selected (based on the distance). ...\n",
    "- The average of these data points is the final prediction for the new point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069b1d7",
   "metadata": {},
   "source": [
    "### 12. How do you choose the value of K in KNN?\n",
    "\n",
    "- The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c2a5c",
   "metadata": {},
   "source": [
    "### 13. What are the advantages and disadvantages of the KNN algorithm?\n",
    "#### Advantages\t\n",
    "1. Simple and Easy to Understand\t\n",
    "2. Non-parametric\t\n",
    "3. No Training Required\t\n",
    "4. Can Handle Large Datasets\t\n",
    "### Disadvantages\n",
    "1. Sensitive to Outliers\n",
    "2. Computationally Expensive\n",
    "3. Requires Good Choice of K\n",
    "4. Limited to Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe86d13",
   "metadata": {},
   "source": [
    "### 14. How does the choice of distance metric affect the performance of KNN?\n",
    "- This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313599aa",
   "metadata": {},
   "source": [
    "### 15. Can KNN handle imbalanced datasets? If yes, how?\n",
    "- In k Nearest Neighbor (kNN) classifier, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3839ff",
   "metadata": {},
   "source": [
    "### 16. How do you handle categorical features in KNN?\n",
    "- You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ee03a",
   "metadata": {},
   "source": [
    "### 17. What are some techniques for improving the efficiency of KNN?\n",
    "\n",
    "- In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4caa52",
   "metadata": {},
   "source": [
    "### 18. Give an example scenario where KNN can be applied.\n",
    "\n",
    "- Speech Recognition\n",
    "- Handwriting Detection\n",
    "- Image Recognition \n",
    "- Video Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4c74e",
   "metadata": {},
   "source": [
    "## Clustering:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff767782",
   "metadata": {},
   "source": [
    "### 19. What is clustering in machine learning?\n",
    "- Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62485e6",
   "metadata": {},
   "source": [
    "### 20. Explain the difference between hierarchical clustering and k-means clustering.\n",
    "\n",
    "- k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'.\n",
    "- Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0a660",
   "metadata": {},
   "source": [
    "### 21. How do you determine the optimal number of clusters in k-means clustering?\n",
    "\n",
    "- The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212f0c2",
   "metadata": {},
   "source": [
    "### 22. What are some common distance metrics used in clustering?\n",
    "- Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. \n",
    "- The four types of distance metrics are \n",
    "1. Euclidean Distance\n",
    "2. Manhattan Distance\n",
    "3. Minkowski Distance\n",
    "4. Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2d541",
   "metadata": {},
   "source": [
    "### 23. How do you handle categorical features in clustering?\n",
    "- Pick K observations at random and use them as leaders/clusters.\n",
    "- Calculate the dissimilarities and assign each observation to its closest cluster.\n",
    "- Define new modes for the clusters.\n",
    "- Repeat 2–3 steps until there are is no re-assignment required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccf851",
   "metadata": {},
   "source": [
    "### 24. What are the advantages and disadvantages of hierarchical clustering?\n",
    "- The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets.\n",
    "- disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500c708",
   "metadata": {},
   "source": [
    "### 25. Explain the concept of silhouette score and its interpretation in clustering.\n",
    "\n",
    "- The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63d3df",
   "metadata": {},
   "source": [
    "### 26. Give an example scenario where clustering can be applied.\n",
    "- clustering can be used to identify several types of depression. \n",
    "- Cluster analysis is also used to identify patterns in the spatial or temporal allocation of a disease.\n",
    "- Business − Businesses collect huge amounts of data on current and potential users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec9700",
   "metadata": {},
   "source": [
    "## Anomaly Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88374cec",
   "metadata": {},
   "source": [
    "### 27. What is anomaly detection in machine learning?\n",
    "- Anomaly detection (aka outlier analysis) is a step in data mining that identifies data points, events, and/or observations that deviate from a dataset's normal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dedb2c",
   "metadata": {},
   "source": [
    "### 28. Explain the difference between supervised and unsupervised anomaly detection.\n",
    "\n",
    "- Supervised learning harnesses the power of labeled data to train models that can make accurate predictions or classifications. In contrast,\n",
    "- unsupervised learning focuses on uncovering hidden patterns and structures within unlabeled data, using techniques like clustering or anomaly detection.29-Jun-2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c048f2",
   "metadata": {},
   "source": [
    "### 29. What are some common techniques used for anomaly detection?\n",
    "- Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "- Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "- Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c3c6f",
   "metadata": {},
   "source": [
    "### 30. How does the One-Class SVM algorithm work for anomaly detection?\n",
    "\n",
    "- One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c2407",
   "metadata": {},
   "source": [
    "### 31. How do you choose the appropriate threshold for anomaly detection?\n",
    "\n",
    "1. In the Upper Limit and Lower Limit fields, specify the upper and lower threshold limits.\n",
    "2. In the Consecutive Occurrences spinner, specify the number of consecutive threshold violations that must occur before an anomaly and an event is generated.\n",
    "3. In the Type drop-down list, specify the threshold type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328bc92",
   "metadata": {},
   "source": [
    "### 32. How do you handle imbalanced datasets in anomaly detection?\n",
    "\n",
    "- Choose Proper Evaluation Metric. The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions. ...\n",
    "- Resampling (Oversampling and Undersampling) ...\n",
    "- SMOTE. ...\n",
    "- BalancedBaggingClassifier. ...\n",
    "- Threshold moving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a19f2",
   "metadata": {},
   "source": [
    "### 33. Give an example scenario where anomaly detection can be applied.\n",
    "\n",
    "\n",
    "- Data Cleaning.\n",
    "- Image Processing.\n",
    "- Fraud and Intrusion Detection.\n",
    "- Threshold-Based Detection in Sensor Networks.\n",
    "- Health Monitoring.\n",
    "- Financial Transactions and Banking Applications.\n",
    "- Ecosystem Turbulences.\n",
    "- IT and Cyber Security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3f0cc",
   "metadata": {},
   "source": [
    "## Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420eca7",
   "metadata": {},
   "source": [
    "### 34. What is dimension reduction in machine learning?\n",
    "- Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874527cf",
   "metadata": {},
   "source": [
    "### 35. Explain the difference between feature selection and feature extraction.\n",
    "\n",
    "- The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf8c22",
   "metadata": {},
   "source": [
    "### 36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "- PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe1b75",
   "metadata": {},
   "source": [
    "### 37. How do you choose the number of components in PCA?\n",
    "- If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a1a98",
   "metadata": {},
   "source": [
    "### 38. What are some other dimension reduction techniques besides PCA?\n",
    "- There are several techniques for dimensionality reduction, including principal component analysis (PCA), singular value decomposition (SVD), and linear discriminant analysis (LDA). Each technique uses a different method to project the data onto a lower-dimensional space while preserving important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe5aeb",
   "metadata": {},
   "source": [
    "### 39. Give an example scenario where dimension reduction can be applied.\n",
    "- Missing values ratio.\n",
    "- High correlation filter.\n",
    "- Recursive Feature Elimination (RFE)\n",
    "- Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee165c",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce3365",
   "metadata": {},
   "source": [
    "### 40. What is feature selection in machine learning?\n",
    "- Feature selection is used to choose a subset of relevant features for effective classification of data. In high dimensional data classification, the performance of a classifier often depends on the feature subset used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b1812",
   "metadata": {},
   "source": [
    "### 41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "\n",
    "- Filter method is faster and useful when there are more number of features. Wrapper method gives better performance while the embedded method lies in between the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71cf5cf",
   "metadata": {},
   "source": [
    "### 42. How does correlation-based feature selection work?\n",
    "\n",
    "- A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074018a7",
   "metadata": {},
   "source": [
    "### 43. How do you handle multicollinearity in feature selection?\n",
    "- Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation.\n",
    "- Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ...\n",
    "- Centering the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b94942",
   "metadata": {},
   "source": [
    "### 44. What are some common feature selection metrics?\n",
    "- There are three general classes of feature selection algorithms: Filter methods, wrapper methods and embedded methods. The role of feature selection in machine learning is, 1. To reduce the dimensionality of feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da9fe7",
   "metadata": {},
   "source": [
    "### 45. Give an example scenario where feature selection can be applied.\n",
    "-  Mammographic image analysis.\n",
    "- Criminal behavior modeling. \n",
    "- Genomic data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ee251",
   "metadata": {},
   "source": [
    "## Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6c324",
   "metadata": {},
   "source": [
    "### 46. What is data drift in machine learning?\n",
    "- Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb94036",
   "metadata": {},
   "source": [
    "### 47. Why is data drift detection important?\n",
    "- Why is data drift important? Data drift is an important concept in machine learning because it can cause a trained model to become less accurate over time, leading to incorrect predictions and suboptimal decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce68732",
   "metadata": {},
   "source": [
    "### 48. Explain the difference between concept drift and feature drift.\n",
    "- Data drift refers to the changing distribution of the data to which the model is applied. \n",
    "- Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac4503",
   "metadata": {},
   "source": [
    "### 49. What are some techniques used for detecting data drift?\n",
    "- Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2fab8",
   "metadata": {},
   "source": [
    "### 50. How can you handle data drift in a machine learning model?\n",
    "- continuously monitoring and evaluating the performance of a model\n",
    "- updating the model with new data\n",
    "- sing machine learning models that are more robust to drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f397198",
   "metadata": {},
   "source": [
    "## Data Leakage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d01af",
   "metadata": {},
   "source": [
    "### 51. What is data leakage in machine learning?\n",
    "- In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1704a3",
   "metadata": {},
   "source": [
    "### 52. Why is data leakage a concern?\n",
    "- Depending on the type of data involved, the consequences can include destruction or corruption of databases, the leaking of confidential information, the theft of intellectual property and regulatory requirements to notify and possibly compensate those affected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c8f7b5",
   "metadata": {},
   "source": [
    "### 53. Explain the difference between target leakage and train-test contamination.\n",
    "- Target leakage can happen when a variable that is not a feature is being used to predict the target. Target leakage happens more frequently with complex datasets. For instance, if the training dataset was normalized or standardized by using missing value imputation (min, max, mean).\n",
    "- Train test contamination happens when we unknowingly or subtly pass information from our train dataset to our validation dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9d0f4",
   "metadata": {},
   "source": [
    "### 54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "- One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab04af3",
   "metadata": {},
   "source": [
    "### 55. What are some common sources of data leakage?\n",
    "- A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by \n",
    "- poor data security and sanitization\n",
    "- outdated systems\n",
    "- a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd5053",
   "metadata": {},
   "source": [
    "### 56. Give an example scenario where data leakage can occur.\n",
    "- Data leakage occurs when sensitive data gets unintentionally exposed to the public in transit, at rest, or in use. \n",
    "####  Here are common examples: \n",
    "- Data exposed in transit — Data transmitted via emails \n",
    "- API calls\n",
    "- chat rooms, and other communications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565c91d",
   "metadata": {},
   "source": [
    "## Cross Validation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef589a7",
   "metadata": {},
   "source": [
    "### 57. What is cross-validation in machine learning?\n",
    "- Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7c01f",
   "metadata": {},
   "source": [
    "### 58. Why is cross-validation important?\n",
    "- The purpose of cross–validation is to test the ability of a machine learning model to predict new data. It is also used to flag problems like overfitting or selection bias and gives insights on how the model will generalize to an independent dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766182e",
   "metadata": {},
   "source": [
    "### 59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "- KFold devides the dataset into k folds.\n",
    "- Stratified ensures that each fold of dataset has the same proportion of observations with a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6c99e",
   "metadata": {},
   "source": [
    "### 60. How do you interpret the cross-validation results?\n",
    "- Cross validation is a technique that allows us to produce test set like scoring metrics using the training set. That is, it allows us to simulate the effects of “going out of sample” using just our training data, so we can get a sense of how well our model generalizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2403e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
