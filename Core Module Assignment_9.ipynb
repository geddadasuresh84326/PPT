{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890e617d",
   "metadata": {},
   "source": [
    "### 1. What is the difference between a neuron and a neural network?\n",
    "#### Neuron :-  \n",
    "- a neuron is the most fundamental unit of processing. It’s also called a perceptron. \n",
    "#### Neural network :- \n",
    "- A neural network is based on the way a human brain works. So, we can say that it simulates the way the biological neurons signal to one another.\n",
    "- A typical neural network consists of layers of neurons called neural nodes. These layers are of the following three types:\n",
    "\n",
    "1. input layer (single)\n",
    "2. hidden layer (one or more than one)\n",
    "3. output layer (single)\n",
    "- Each neural node is connected to another and is characterized by its weight and a threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd818fa",
   "metadata": {},
   "source": [
    "### 2. Can you explain the structure and components of a neuron?\n",
    "- Neurons in deep learning models are nodes through which data and computations flow.\n",
    "\n",
    "#### Neurons work like this:\n",
    "\n",
    "- They receive one or more input signals. These input signals can come from either the raw data set or from neurons positioned at a previous layer of the neural net.\n",
    "- They perform some calculations.\n",
    "- They send some output signals to neurons deeper in the neural net through a synapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa233dc",
   "metadata": {},
   "source": [
    "### 3. Describe the architecture and functioning of a perceptron.\n",
    "\n",
    "- Perceptron is considered a single-layer neural link with four main parameters.\n",
    "- The perceptron model begins with multiplying all input values and their weights, then adds these values to create the weighted sum. \n",
    "- Further, this weighted sum is applied to the activation function ‘f’ to obtain the desired output.\n",
    "- This activation function is also known as the step function and is represented by ‘f.’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1f34e",
   "metadata": {},
   "source": [
    "### 4. What is the main difference between a perceptron and a multilayer perceptron?\n",
    "- A perceptron is a simple type of neural network that can learn to classify linearly separable patterns. It consists of a single layer of weighted inputs and a binary output.\n",
    "- A multi-layer perceptron (MLP) is a more complex type of neural network that can learn to classify non-linearly separable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cf534",
   "metadata": {},
   "source": [
    "### 5. Explain the concept of forward propagation in a neural network.\n",
    "- Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdd71a",
   "metadata": {},
   "source": [
    "### 6. What is backpropagation, and why is it important in neural network training?\n",
    "- Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights. Backpropagation is the essence of neural net training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29726e",
   "metadata": {},
   "source": [
    "### 7. How does the chain rule relate to backpropagation in neural networks?\n",
    "\n",
    "- The chain rule can be generalised to multivariate functions, and represented by a tree diagram. The chain rule is applied extensively by the backpropagation algorithm in order to calculate the error gradient of the loss function with respect to each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b08eb1",
   "metadata": {},
   "source": [
    "### 8. What are loss functions, and what role do they play in neural networks?\n",
    "\n",
    "- A loss function is a function that compares the target and predicted output values; measures how well the neural network models the training data. When training, we aim to minimize this loss between the predicted and target outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3902b3",
   "metadata": {},
   "source": [
    "### 9. Can you give examples of different types of loss functions used in neural networks?\n",
    "1. Mean Absolute Error (L1 Loss)\n",
    "2. Mean Squared Error (L2 Loss)\n",
    "3. Huber Loss.\n",
    "4. Cross-Entropy(a.k.a Log loss)\n",
    "5. Relative Entropy(a.k.a Kullback–Leibler divergence)\n",
    "6. Squared Hinge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48841dc",
   "metadata": {},
   "source": [
    "### 10. Discuss the purpose and functioning of optimizers in neural networks.\n",
    "- An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d832224",
   "metadata": {},
   "source": [
    "### 11. What is the exploding gradient problem, and how can it be mitigated?\n",
    "- The exploding gradient problem is one of the main barriers to training deep neural networks. It is widely believed that this problem can be solved by techniques like careful weight initialization and normalization layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971848b4",
   "metadata": {},
   "source": [
    "### 12. Explain the concept of the vanishing gradient problem and its impact on neural network training.\n",
    "- Vanishing gradient problem is a phenomenon that occurs during the training of deep neural networks, where the gradients that are used to update the network become extremely small or \"vanish\" as they are backpropogated from the output layers to the earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03b576",
   "metadata": {},
   "source": [
    "### 13. How does regularization help in preventing overfitting in neural networks?\n",
    "- Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8043c7",
   "metadata": {},
   "source": [
    "### 14. Describe the concept of normalization in the context of neural networks.\n",
    "- Normalizing a set of data transforms the set of data to be on a similar scale. For machine learning models, our goal is usually to recenter and rescale our data such that is between 0 and 1 or -1 and 1, depending on the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb793c",
   "metadata": {},
   "source": [
    "### 15. What are the commonly used activation functions in neural networks?\n",
    "- Linear or Identity Activation Function.\n",
    "- Non-linear Activation Function.\n",
    "- Sigmoid or Logistic Activation Function.\n",
    "- Tanh or hyperbolic tangent Activation Function.\n",
    "- ReLU (Rectified Linear Unit) Activation Function.\n",
    "- Leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee0508",
   "metadata": {},
   "source": [
    "### 16. Explain the concept of batch normalization and its advantages.\n",
    "- Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly. Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585dd69",
   "metadata": {},
   "source": [
    "### 17. Discuss the concept of weight initialization in neural networks and its importance.\n",
    "- Weight initialization is a procedure to set the weights of a neural network to small random values that define the starting point for the optimization (learning or training) of the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c252264",
   "metadata": {},
   "source": [
    "### 18. Can you explain the role of momentum in optimization algorithms for neural networks?\n",
    "- Neural network momentum is a simple technique that often improves both training speed and accuracy. Training a neural network is the process of finding values for the weights and biases so that for a given set of input values, the computed output values closely match the known, correct, target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d50f24",
   "metadata": {},
   "source": [
    "### 19. What is the difference between L1 and L2 regularization in neural networks?\n",
    "\n",
    "- L1and L2 are two loss functions in machine learning which are used to minimize the error.\n",
    "\n",
    "- L1 Loss function stands for Least Absolute Deviations.\n",
    "\n",
    "- L2 Loss function stands for Least Square Errors.\n",
    "\n",
    "- Generally, L2 Loss Function is preferred in most of the cases. But when the outliers are present in the dataset, then the L2 Loss Function does not perform well. The reason behind this bad performance is that if the dataset is having outliers, then because of the consideration of the squared differences, it leads to the much larger error. Hence, L2 Loss Function is not useful here. Prefer L1 Loss Function as it is not affected by the outliers or remove the outliers and then use L2 Loss Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7d9aa",
   "metadata": {},
   "source": [
    "### 20. How can early stopping be used as a regularization technique in neural networks?\n",
    "- Regularization by early stopping can be done either by dividing the dataset into training and test sets and then using cross-validation on the training set or by dividing the dataset into training, validation and test sets, in which case cross-validation, is not required. Here, the second case is analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64b1cf",
   "metadata": {},
   "source": [
    "### 21. Describe the concept and application of dropout regularization in neural networks.\n",
    "- Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf447fa",
   "metadata": {},
   "source": [
    "### 22. Explain the importance of learning rate in training neural networks.\n",
    "- In neural network models, the learning rate is a crucial hyperparameter that regulates the magnitude of weight updates applied during training. It is crucial in influencing the rate of convergence and the caliber of a model's answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a473ebb",
   "metadata": {},
   "source": [
    "### 23. What are the challenges associated with training deep neural networks?\n",
    "- This can lead to several issues, such as the inability of the network to learn, poor generalization, and slow convergence. There are several common activation functions that can suffer from non-saturation, including the linear activation function and the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d0c7e",
   "metadata": {},
   "source": [
    "### 24. How does a convolutional neural network (CNN) differ from a regular neural network?\n",
    "- RNNs are better suited to analyzing temporal, sequential data, such as text or videos. A CNN has a different architecture from an RNN. CNNs are \"feed-forward neural networks\" that use filters and pooling layers, whereas RNNs feed results back into the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289e768",
   "metadata": {},
   "source": [
    "### 25. Can you explain the purpose and functioning of pooling layers in CNNs?\n",
    "- Pooling layers are used to reduce the dimensions of the feature maps. \n",
    "- Thus, it reduces the number of parameters to learn and the amount of computation performed in the network.\n",
    "- The pooling layer summarises the features present in a region of the feature map generated by a convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068cf7d",
   "metadata": {},
   "source": [
    "### 26. What is a recurrent neural network (RNN), and what are its applications?\n",
    "- Recurrent Neural Networks enable you to model time-dependent and sequential data problems, such as stock market prediction, machine translation, and text generation. \n",
    "- You will find, however, RNN is hard to train because of the gradient problem. \n",
    "- RNNs suffer from the problem of vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8867e",
   "metadata": {},
   "source": [
    "### 27. Describe the concept and benefits of long short-term memory (LSTM) networks.\n",
    "- Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8aa29",
   "metadata": {},
   "source": [
    "### 28. What are generative adversarial networks (GANs), and how do they work?\n",
    "\n",
    "- Generative Adversarial Networks (GANs) were introduced in 2014 by Ian J. Goodfellow and co-authors. GANs perform unsupervised learning tasks in machine learning. It consists of 2 models that automatically discover and learn the patterns in input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c63add",
   "metadata": {},
   "source": [
    "### 29. Can you explain the purpose and functioning of autoencoder neural networks?\n",
    "- autoencoders are used to help reduce the noise in data. Through the process of compressing input data, encoding it, and then reconstructing it as an output, autoencoders allow you to reduce dimensionality and focus only on areas of real value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee80da",
   "metadata": {},
   "source": [
    "### 30. Discuss the concept and applications of self-organizing maps (SOMs) in neural networks.\n",
    "- Self-Organizing Maps(SOMs) are a form of unsupervised neural network that are used for visualization and exploratory data analysis of high dimensional datasets. Our goal was to understand how we can use a SOM to gain insights about datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd4c51",
   "metadata": {},
   "source": [
    "### 31. How can neural networks be used for regression tasks?\n",
    "- The purpose of using Artificial Neural Networks for Regression over Linear Regression is that the linear regression can only learn the linear relationship between the features and target and therefore cannot learn the complex non-linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4822c9",
   "metadata": {},
   "source": [
    "### 32. What are the challenges in training neural networks with large datasets?\n",
    "- Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs.\n",
    "- The problem is hard, not least because the error surface is non-convex and contains local minima, flat spots, and is highly multidimensional.\n",
    "- The stochastic gradient descent algorithm is the best general algorithm to address this challenging problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29136cf7",
   "metadata": {},
   "source": [
    "### 33. Explain the concept of transfer learning in neural networks and its benefits.\n",
    "- Transfer learning is the application of knowledge gained from completing one task to help solve a different, but related, problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a9aed",
   "metadata": {},
   "source": [
    "### 34. How can neural networks be used for anomaly detection tasks?\n",
    "- Anomaly detection is identifying data points in data that don't fit the normal patterns. It can be useful to solve many problems including fraud detection, medical diagnosis, etc. Machine learning methods allow to automate anomaly detection and make it more effective, especially when large datasets are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd6a2c",
   "metadata": {},
   "source": [
    "### 35. Discuss the concept of model interpretability in neural networks.\n",
    "- A model with high accuracy is what we usually call a good model, it learned the relationship between the inputs X and outputs y well. If a model has high interpretability or explainability, we understand how the model makes a prediction and how we can influence this prediction by changing input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553cf92",
   "metadata": {},
   "source": [
    "### 36. What are the advantages and disadvantages of deep learning compared to traditional machine learning algorithms?\n",
    "- Deep learning has several advantages over traditional machine learning methods, including automatic feature learning, handling large and complex data, improved performance, handling non-linear relationships, handling structured and unstructured data, predictive modeling, handling missing data, handling sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9d859",
   "metadata": {},
   "source": [
    "### 37. Can you explain the concept of ensemble learning in the context of neural networks?\n",
    "- Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error. Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb77679",
   "metadata": {},
   "source": [
    "### 38. How can neural networks be used for natural language processing (NLP) tasks?\n",
    "- deep learning models and learning techniques based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) enable NLP systems that 'learn' as they work and extract ever more accurate meaning from huge volumes of raw, unstructured, and unlabeled text and voice data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1b1a7",
   "metadata": {},
   "source": [
    "### 39. Discuss the concept and applications of self-supervised learning in neural networks.\n",
    "- Self-supervised learning is a machine learning paradigm that processes unlabeled data to obtain useful representations that can help with downstream learning tasks. \n",
    "- It is particularly suitable for speech recognition¹. In this technique, the unsupervised problem is changed into a supervised one using auto-generation of labels². The output labels are a part of the input data, thus no separate output labels are required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e203bfb",
   "metadata": {},
   "source": [
    "### 40. What are the challenges in training neural networks with imbalanced datasets?\n",
    "-  When the dataset has underrepresented data, the class distribution starts skew. Due to the inherent complex characteristics of the dataset, learning from such data requires new understandings, new approaches, new principles, and new tools to transform data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cae9a1",
   "metadata": {},
   "source": [
    "### 41. Explain the concept of adversarial attacks on neural networks and methods to mitigate them.\n",
    "- An adversarial attack is a method of making small modifications to the objects in such a way that the machine learning model begins to misclassify them. Neural networks (NN) are known to be vulnerable to such attacks. Research of adversarial methods historically started in the sphere of image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5053ae",
   "metadata": {},
   "source": [
    "### 42. Can you discuss the trade-off between model complexity and generalization performance in neural networks?\n",
    "- One of the most important trade-offs is between complexity and generalization. Complexity refers to how well a model can fit the data and capture the nuances and patterns. Generalization refers to how well a model can perform on new and unseen data and avoid overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcd5f6",
   "metadata": {},
   "source": [
    "### 43. What are some techniques for handling missing data in neural networks?\n",
    "####  Popular strategies to handle missing values in the dataset\n",
    "- Deleting Rows with missing values.\n",
    "- Impute missing values for continuous variable.\n",
    "- Impute missing values for categorical variable.\n",
    "- Other Imputation Methods.\n",
    "- Using Algorithms that support missing values.\n",
    "- Prediction of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3108284",
   "metadata": {},
   "source": [
    "### 44. Explain the concept and benefits of interpretability techniques like SHAP values and LIME in neural networks.\n",
    "- SHAP stands for “SHapley Additive exPlanations.” Shapley values are a widely used approach from cooperative game theory. The essence of Shapley value is to measure the contributions to the final outcome from each player separately among the coalition, while preserving the sum of contributions being equal to the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb07006",
   "metadata": {},
   "source": [
    "### 45. How can neural networks be deployed on edge devices for real-time inference?\n",
    "- There are three major techniques for achieving on-device computation and storage. The first technique is quantization, which uses a few bits to represent each weight and activation in a DNN model. The second technique is pruning by eliminating redundant connections and neurons in a DNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519a9d8",
   "metadata": {},
   "source": [
    "### 46. Discuss the considerations and challenges in scaling neural network training on distributed systems.\n",
    "- One of the main challenges of neural networks and deep learning is the need for large amounts of data and computational resources. Neural networks learn from data by adjusting their parameters to minimize a loss function, which measures how well they fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ce8ac",
   "metadata": {},
   "source": [
    "### 47. What are the ethical implications of using neural networks in decision-making systems?\n",
    "- Lack of transparency of AI tools: AI decisions are not always intelligible to humans.\n",
    "- AI is not neutral: AI-based decisions are susceptible to inaccuracies, discriminatory outcomes, embedded or inserted bias. - - Surveillance practices for data gathering and privacy of court users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e926e52",
   "metadata": {},
   "source": [
    "### 48. Can you explain the concept and applications of reinforcement learning in neural networks?\n",
    "- Reinforcement learning is a machine learning training method based on rewarding desired behaviors and/or punishing undesired ones. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7b89b",
   "metadata": {},
   "source": [
    "### 49. Discuss the impact of batch size in training neural networks.\n",
    "\n",
    "- The batch size affects some indicators such as overall training time, training time per epoch, quality of the model, and similar. Usually, we chose the batch size as a power of two, in the range between 16 and 512. But generally, the size of 32 is a rule of thumb and a good initial choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0fb41",
   "metadata": {},
   "source": [
    "### 50. What are the current limitations of neural networks and areas for future research?\n",
    "\n",
    "- Neural networks are vulnerable to subtle perturbations or modifications of the input data, which can cause them to produce incorrect or misleading outputs. For example, adding a small amount of noise or changing a few pixels in an image can fool a neural network into misclassifying it as a different object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
